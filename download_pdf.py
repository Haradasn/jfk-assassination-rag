import requests
from bs4 import BeautifulSoup
import os
import re
import time
from tqdm import tqdm
from urllib.parse import urljoin

# è¨­å®š
url = 'https://www.archives.gov/research/jfk/release-2025'
save_dir = 'jfk_pdfs'
os.makedirs(save_dir, exist_ok=True)

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'
}

# å–å¾—é–¢æ•°ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
def download_file(pdf_url, save_path, retries=3):
    for attempt in range(retries):
        try:
            response = requests.get(pdf_url, headers=headers, timeout=30)
            if response.status_code == 200:
                with open(save_path, 'wb') as f:
                    f.write(response.content)
                return True
            else:
                print(f"âš ï¸ Status code {response.status_code}: {pdf_url}")
        except requests.exceptions.Timeout:
            print(f"â³ Timeout: {pdf_url}")
        except Exception as e:
            print(f"âŒ Error: {e}")
        time.sleep(2)  # ãƒªãƒˆãƒ©ã‚¤å‰ã«ã¡ã‚‡ã£ã¨ä¼‘æ†©
    return False

# ãƒšãƒ¼ã‚¸ã‹ã‚‰PDFãƒªãƒ³ã‚¯æŠ½å‡º
print("ğŸ” PDFãƒªãƒ³ã‚¯æŠ½å‡ºä¸­...")
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')

pdf_links = set()
for a_tag in soup.find_all('a', href=True):
    href = a_tag['href']
    if re.search(r'\.pdf$', href, re.IGNORECASE):
        full_url = urljoin(url, href)
        pdf_links.add(full_url)

print(f"âœ… è¦‹ã¤ã‹ã£ãŸPDFæ•°: {len(pdf_links)}")

# PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹ï¼ˆé€²æ—ä»˜ãï¼‰
for pdf_url in tqdm(pdf_links, desc="ğŸ“¥ Downloading PDFs"):
    filename = os.path.join(save_dir, os.path.basename(pdf_url))
    success = download_file(pdf_url, filename)
    if not success:
        print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {pdf_url}")
    time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã1ç§’ä¼‘æ†©

print("ğŸ‰ å…¨PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")